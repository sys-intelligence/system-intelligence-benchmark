# Submission API
API_HOSTNAME="0.0.0.0"
API_PORT="8000"

# MCP Server Setting
MCP_SERVER_PORT=8001
EXPOSE_SERVER=False
# The size of session that can be hold at most; LRU session will be evicted if overflowing
SESSION_CACHE_SIZE=10000
# The time after which the session will be considered inactive if no interaction with the server;
# Cache of inactive session will be deleted in the next cache mutating operation
SESSION_TTL=600

# Timeout in seconds for waiting for pods to become ready during deployment.
# Increase this value if you have slow network connections (e.g., first deployment).
# Default: 600 (10 minutes)
# Recommended for slow networks: 1800 (30 minutes)
WAIT_FOR_POD_READY_TIMEOUT=600

# LangGraph Tool Setting
MCP_SERVER_URL=http://127.0.0.1:${MCP_SERVER_PORT}

# Agent LLM Config
# Maximum number of retries for API calls
LLM_QUERY_MAX_RETRIES=5
# Initial delay in seconds for retries
LLM_QUERY_INIT_RETRY_DELAY=1


### LLM Config1: LiteLLM ###
# PROVIDER_TOOLS="litellm" 
# PROVIDER="litellm"

# MODEL_TOOLS="gemini/gemini-2.5-flash"
# MODEL_TOOLS="openai/gpt-4o"
# MODEL_TOOLS="anthropic/claude-sonnet-4-20250514" # this one is ok, but the model on the doc of LiteLLM seems to be invalid. 
# MODEL_TOOLS="bedrock/meta.llama3-1-70b-instruct-v1:0"

# GEMINI_API_KEY="AIHaveFreeFood_LotsOfIt"
# OPENAI_API_KEY="sk-proj-HaveSleep_LotsOfIt"
# ANTHROPIC_API_KEY="sk-ant-api03-HaveCats_LotsOfIt_Meow"

# AWS_PROFILE="default"
# AWS_DEFAULT_REGION=us-east-2


### LLM Config2: WatsonX ###
# PROVIDER_TOOLS="watsonx"
# PROVIDER="watsonx"
# MODEL_TOOLS="meta-llama/llama-3-3-70b-instruct"
# URL_TOOLS="https://us-south.ml.cloud.ibm.com"
# API_KEY_TOOLS="HaveCornsLotsOfIt"
# WATSONX_API_BASE="https://us-south.ml.cloud.ibm.com"
# WX_PROJECT_ID="fe3d8da2-be7e-41b2-8d92-f0e855869e28"
# WATSONX_API_KEY="HaveCornsLotsOfIt"

### LLM Config3: LiteLLM not supported but OpenAI API compatible ###
# PROVIDER="compatible"
# PROVIDER_TOOLS="compatible"

# MODEL_TOOLS="openai/glm-4" # you should use 'openai' as the provider to reuse the interface
# API_KEY_TOOLS="HaveFunding_LotsOfIt"
# URL_TOOLS="https://open.bigmodel.cn/api/paas/v4/" # the base url of the model, taking Zhipu GLM as the example

